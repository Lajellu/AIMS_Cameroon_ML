{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab10","provenance":[{"file_id":"17RIs1SRtCkmU5sLZVC4KtnFF2Luur92W","timestamp":1609226556879},{"file_id":"https://github.com/ak9250/gpt-2-colab/blob/master/GPT_2.ipynb","timestamp":1609221708502}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AhQsu-RQQXWG"},"source":["# GPT-2 Example\r\n","\r\n","## Here we will run through the popular GPT-2:\r\n","\r\n","GPT-2 is a neural-network-powered language model. A language model is a model that predicts the likelihood of a sentence existing in the world. For example, a language model can label the sentence “I take my dog for a walk” as more probable to exist (i.e., on the Internet) than the sentence “I take my banana for a walk.” This is true for sentences as well as phrases and, more generally, any sequence of characters.\r\n","\r\n","Like most language models, GPT-2 is elegantly trained on an unlabeled text dataset (in this case, the training data includes among others Common Crawl and Wikipedia). Words or phrases are randomly removed from the text, and the model must learn to fill them in using only the surrounding words as context. It’s a simple training task that results in a powerful and generalizable model.\r\n","\r\n","\r\n","---\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"Pzxl1vYX-1kk"},"source":["Setup:\n","\n","1) Make sure GPU is enabled, go to edit->notebook settings->Hardware Accelerator GPU\n","\n","2) Make a copy to your google drive, click on copy to drive in panel"]},{"cell_type":"markdown","metadata":{"id":"iW0abT07ZkhZ"},"source":["Note: Colab will reset after 12 hours make sure to save your model checkpoints to google drive around 10-11 hours mark or before, then go to runtime->reset all runtimes. Now copy your train model back into colab and start training again from the previous checkpoint."]},{"cell_type":"markdown","metadata":{"id":"iLXW02eIYpcB"},"source":["clone and cd into repo\r\n","\r\n","\r\n","---\r\n","# FOR NOW I AM USING MY PUBLIC REPO CLONE - WE CAN CLONE INTO THE CAMEROON GITHUB>?\r\n","\r\n","\r\n","---\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"ICYu3w9hIJkC"},"source":["!git clone https://github.com/MissCrispenCakes/GPT2_example_CAMEROON.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6eEIs3ApZUVO"},"source":["cd GPT2_example_CAMEROO\\N"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qtn1qZPgZLb0"},"source":["Install requirements.\r\n","\r\n","We will use an earlier version of TensorFlow than we have been using for our labs so far - this will make it easier for you to play around with the introductory content that exists online. (If time will update/transfer to newer TF - don't worry about the 'incompatible' flags - they get resolved)"]},{"cell_type":"code","metadata":{"id":"434oOx0bZH6J"},"source":["!pip3 install tensorflow==1.15.0rc3\r\n","!pip3 install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WvUQhgK3PQ4L"},"source":["Mount drive to access google drive for saving and accessing checkpoints later. Have to log in to your google account - you may already be connected from Lab 9, run this anyway to check."]},{"cell_type":"code","metadata":{"id":"FNpf6R4ahYSN"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1hrgeKFYsuE"},"source":["Download the model data - we will only consider pre-trained models that are small and medium in size (due to time, space)\r\n","\r\n","*   117M\r\n","*   345M\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"A498TySgHYyF"},"source":["!python3 download_model.py 117M"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5UDpEGjfO8Q2"},"source":["!python3 download_model.py 345M"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zq-YwRnNOBYO"},"source":["encoding"]},{"cell_type":"code","metadata":{"id":"7oJPQtdLbbeK"},"source":["!export PYTHONIOENCODING=UTF-8"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KzSbAvePgsI"},"source":["Fetch checkpoints if you have them saved in google drive -  if you don't have a saved checkpoint yet DON'T worry! The following commands will still run and check the drive but won't find anything yet."]},{"cell_type":"code","metadata":{"id":"cA2Wk7yIPmS6"},"source":["!cp -r /content/drive/My\\Drive/checkpoint/ /content/GPT2_example_CAMEROO\\N// "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0p--9zwqQRTc"},"source":["\n","Let's get our train on! In this case the file is A Tale of Two Cities (Charles Dickens) from Project Gutenberg. To change the dataset GPT-2 models will fine-tune on, change this URL to another .txt file, and change corresponding part of the next cell. Note that you can use small datasets if you want but you will have to be sure not to run the fine-tuning for too long or you will overfit badly. Roughly, expect interesting results within minutes to hours in the 1-10s of megabyte ballpark, and below this you may want to stop the run early as fine-tuning can be very fast."]},{"cell_type":"code","metadata":{"id":"QOCvrs-DHvxa"},"source":["!wget https://www.gutenberg.org/files/98/98-0.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yPfJ5b3CQXqr"},"source":["\n","Start training, add --model_name '345M' to use 345 model. Use 117M for the smaller model. Again, as we are using an older version of TensorFlow there will be some 'deprecated' flags; we can ignore these. You can stop the training at anypoint with CTRL + C _OR_ you can right click on the three dots to the left of the output below and select \"interrupt execution\" (the three dots appear directly below the spinning stop/play button beside the code); the training will continue 'indefinitely' and will populate the screen with samples in intervals of 100. To get a feel and gist for the system, stop the training after first 200 and move on. You can always come back as we have set up checkpoints!"]},{"cell_type":"code","metadata":{"id":"pEn_ihcGI00T"},"source":["!chmod u+rwx train.py\r\n","!PYTHONPATH=src ./train.py --dataset /content/GPT2_example_CAMEROO\\N//98-0.txt --model_name '345M'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vS1RJJDFOPnb"},"source":["Save our checkpoints to start training again later"]},{"cell_type":"code","metadata":{"id":"JretqG1zOXdi"},"source":["!cp -r /content/GPT2_example_CAMEROO\\N/checkpoint /content/drive/My\\ Drive/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6D-i7vERWbNS"},"source":["Load your trained model for use in sampling below (117M or 345M) -  if you don't have a trained model yet DON'T worry! The following commands will still run and check the drive but won't find anything yet. If you have only trained 117M or only trained 345M - you will only be able to load one of, not both of the following load codes."]},{"cell_type":"code","metadata":{"id":"VeETvWvrbKga"},"source":["!cp -r /content/GPT2_example_CAMEROO\\N/checkpoint/run1/* /content/GPT2_example_CAMEROO\\N/models/117M/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"np0r6qfXBeUX"},"source":["!cp -r /content/GPT2_example_CAMEROO\\N/checkpoint/run1/* /content/GPT2_example_CAMEROO\\N/models/345M/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GmnSrXqtfRbq"},"source":["Now we will generate conditional samples from the model given a prompt you provide -  change top-k hyperparameter if desired (default is 40),  if you're using 345M, add \"--model-name 345M\"\r\n","\r\n","# Have fun with the model inputs! Try words, sentences, paragraphs!\r\n","## You can enter whatever comes to mind! Ask a question. Give a comment. Paste in a passage from a book. Get creative!\r\n","\r\n","The prompt will appear at the bottom of the output section, wait for it, enter your text and hit 'enter' on your keyboard to 'communicate' with your trained model. As before, you can 'interrupt execution' to move on. You can return back at anytime by running this section of code again."]},{"cell_type":"code","metadata":{"id":"utJj-iY4gHwE"},"source":["!chmod u+rwx src/interactive_conditional_samples.py\r\n","!python3 src/interactive_conditional_samples.py --top_k 40 --model_name \"345M\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LeDhY97XMDXn"},"source":["To check flag descriptions, use:"]},{"cell_type":"code","metadata":{"id":"pBaj2L_KMAgb"},"source":["!python3 src/interactive_conditional_samples.py -- --help"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K8rSqkGxg5OK"},"source":["An alternative to interactive sample generation is the following, again \"interrupt execution\" when you are ready to move on:\r\n","\r\n","Generate unconditional samples from the model,  if you're using 345M, add \"--model-name 345M\""]},{"cell_type":"code","metadata":{"id":"LaQUEnRxWc3c"},"source":["!chmod u+rwx src/generate_unconditional_samples.py\r\n","!python3 src/generate_unconditional_samples.py --model_name \"345M\" | tee /tmp/samples"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VM1Hag-JL3Bt"},"source":["To check flag descriptions, use:"]},{"cell_type":"code","metadata":{"id":"Sdxfye-SL66I"},"source":["!python3 src/generate_unconditional_samples.py -- --help"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NijKHK0_OPhs"},"source":[""],"execution_count":null,"outputs":[]}]}